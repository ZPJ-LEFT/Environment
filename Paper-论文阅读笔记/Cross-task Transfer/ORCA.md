# Cross-Modal Fine-Tuning: Align then Refine

## 摘要
预训练大模型微调已经成为NLP、CV领域的一种高效的技术，然而在其他数据有限的模态上预训练大模型是缺失的，无法实现大模型微调。

本论文提出ORCA，一种通用跨模态微调框架，它将一个大规模预训练模型拓展到其他模态上。具体而言，ORCA遵循algin-then-refine的流程：
- 给定目标输入，ORCA首先学习嵌入网络将预训练模态与嵌入特征分布对齐
- 预训练模型在嵌入数据上微调，以利用模态间共享的知识

实验证明，ORCA在包含60个数据集、12个模态的3个Benchmark上取得了SOTA，优于被广泛应用的手动设计、AutoML、通用的、任务特定的方法。

## 引言

（手动设计、AutoML）在某些领域设计专用网络是具有挑战性的，因为他需要领域专业知识，利用自动机器学习、通用架构可以简化该过程，但对于数据稀缺的领域而言从零开始训练网络仍是困难的。

（通用的、任务特定的）近期工作证明，预训练的大语言模型可以被用于视觉任务，但这些工作都是特别设计的，依赖于手动提示词工程或附加组件来解决特定任务，并且通常比不过从零开始训练的模型。

ORCA的关键见解是先执行任务特定的数据对齐，然后执行任务无关的微调。它遵循三阶段的工作流：

（1）生成一个任务特定的嵌入网络，将目标输入映射成序列特征
（2）训练嵌入网络，最小化嵌入目标特征、源特征之间的分布距离
（3）微调整个模型

## 相关工作

（1）AutoML/NAS：搜索最佳的网络架构并从头开始训练网络
（2）Unimodal domain adaptation (DA): 假设源任务和目标任务相同，但是源域和目标域的数据分布不同，例如检测文字的OCR迁移到检测数字
（3）Unimodal fine-tuning：例如将预训练的大语言模型迁移到其他语言相关任务
（4）Multimodal fine-tuning：多种模态联合作为输入，然后将其迁移到其他相关任务，输入仍是规定的模态，例如图像、语言结合的大模型
（5）General-purpose models：适用多种任务的通用架构，这些方法利用不同任务的大量数据训练多任务Transformer
（6）Heterogeneous DA：假设源任务和目标任务相同，对不同的输入进行迁移，例如将文本分类模型迁移到图像分类模型，以及不同分辨率的图像输入
（7）Cross-modal task-specific fine-tuning: 将大语言模型迁移到其他模态的任务，例如视觉任务，它们依赖于对单个模态的手工设计
（8）Frozen Pretrained Transformers (FPT): 相较于ORCA，缺少阶段2，即没有考虑模态的迁移

## 方法

#### 架构设计
首先需要解决维数不匹配问题，将transformer-based learner (m)分为三部分：embedder(f)、body(g)、predictor(h)。

（1）自定义嵌入网络
（2）预训练Transformer主干
（3）自定义预测头：利用Pxielshuffle实行上采样

#### 分布对齐的嵌入学习
直观而言，从相似模态进行知识迁移比在相异模态进行更加简单。

论文中，在微调模型前训练一个嵌入器，使目标特征与源特征尽量相似。

#### 下游任务适应的微调

